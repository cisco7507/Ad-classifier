# =============================================================================
# Video Ad Classifier — Environment Configuration
# Copy this file to .env and edit values before running.
# =============================================================================

# ── Node identity ─────────────────────────────────────────────────────────────
# Unique name for this node (used as job ID prefix).
NODE_NAME=node-a

# Port the Uvicorn API server listens on.
PORT=8000

# ── Database ──────────────────────────────────────────────────────────────────
# SQLite database file path. Each node in a cluster must point to its OWN file.
# If omitted, service defaults to video_service_${NODE_NAME}.db.
DATABASE_PATH=video_service.db

# SQLite connection timeout (seconds) and busy timeout (milliseconds).
# Increase only if lock contention persists under high write load.
SQLITE_TIMEOUT_SECONDS=30
SQLITE_BUSY_TIMEOUT_MS=30000

# Taxonomy CSV used to map Category -> Category ID.
# Default (if omitted): <repo>/video_service/data/categories.csv
#CATEGORY_CSV_PATH=/app/video_service/data/categories.csv

# ── Cluster config ────────────────────────────────────────────────────────────
# Path to the JSON file defining cluster nodes (see cluster_config.json).
# Leave unset for single-node mode — the node registers itself automatically.
# CLUSTER_CONFIG=cluster_config.json

# ── Storage paths ─────────────────────────────────────────────────────────────
# Temporary directory for uploaded video files.
UPLOAD_DIR=/tmp/video_service_uploads

# Persistent directory for per-job artifacts (extracted frames, etc.)
ARTIFACTS_DIR=/tmp/video_service_artifacts

# ── Security ──────────────────────────────────────────────────────────────────
# Maximum upload size in MB (default: 500).
MAX_UPLOAD_MB=5000

# Maximum URL-fetched content size in MB (default: 2048).
URL_MAX_SIZE_MB=2048

# HTTP timeout in seconds when fetching a video URL (default: 30).
URL_FETCH_TIMEOUT=30

# Optional comma-separated hostname allowlist. If set, only these hosts are
# allowed in /jobs/by-urls submissions. Empty = allow all.
# URL_HOST_ALLOWLIST=mycompany.com,cdn.mycompany.com

# Optional comma-separated hostname denylist. These hosts are always rejected.
# URL_HOST_DENYLIST=localhost,127.0.0.1,169.254.0.0/16

# Comma-separated absolute paths that the /jobs/by-folder endpoint may scan.
# If empty, any absolute server-side path is accepted (less secure).
# ALLOWED_FOLDER_ROOTS=/data/videos,/mnt/nas/ads

# ── CORS ──────────────────────────────────────────────────────────────────────
# Comma-separated list of allowed origins for the dashboard.
# Adjust to your dashboard URL in production.
CORS_ORIGINS=http://localhost:5173,http://localhost:5174,http://127.0.0.1:5173,http://127.0.0.1:5174

# ── Cleanup ───────────────────────────────────────────────────────────────────
# Enable/disable background cleanup thread (true/false).
CLEANUP_ENABLED=true

# Delete completed/failed job rows older than this many days.
JOB_TTL_DAYS=30

# How often the cleanup job runs (hours).
CLEANUP_INTERVAL_HOURS=6

# ── Stale Job Recovery ───────────────────────────────────────────────────────
# Jobs stuck in 'processing' longer than this (seconds) are reset to 'queued'.
# Set to 0 to disable the continuous watchdog (startup recovery still runs).
STALE_JOB_TIMEOUT_SECONDS=180

# How often the watchdog checks for stale jobs (seconds).
STALE_JOB_CHECK_INTERVAL_SECONDS=120

# ── Workers ───────────────────────────────────────────────────────────────────
# When true, the API process spawns worker processes on startup.
# Set to false to run workers separately via:
#   python -m video_service.workers.worker
EMBED_WORKERS=true

# Number of worker PROCESSES to spawn when you run:
#   python -m video_service.workers.worker
# Set >1 to run multiple workers from a single terminal.
# Example: WORKER_PROCESSES=4 starts 4 worker processes under one supervisor.
# If omitted/invalid, defaults to 1.
WORKER_PROCESSES=1

# Internal pipeline threads used within ONE job.
# Keep this at 1 on MPS/CPU to avoid memory pressure.
# Increase only if a single job contains many inputs and hardware can handle it.
PIPELINE_THREADS_PER_JOB=1

# ── Logging ───────────────────────────────────────────────────────────────────
# Python log level (DEBUG, INFO, WARNING, ERROR).
LOG_LEVEL=INFO

# ── Device / ML ───────────────────────────────────────────────────────────────
# Torch device override. Leave empty for auto-detect (MPS > CUDA > CPU).
# TORCH_DEVICE=cpu

# ── Ollama ────────────────────────────────────────────────────────────────────
# Base URL of the Ollama server. Include protocol and port, no trailing slash.
OLLAMA_HOST=http://localhost:11434

# ── Watch-Folder ──────────────────────────────────────────────────────────────
# Comma-separated directories to watch for incoming video files.
# Leave empty to disable watch-folder ingest entirely.
# WATCH_FOLDERS=/mnt/nas/incoming_ads,/data/new_videos
WATCH_FOLDERS=

# Directory where completed/failed watch-folder job results are written as JSON.
# Each result file is named: <source_video_filename>.json
# Leave empty to disable JSON result export.
# WATCH_OUTPUT_DIR=/mnt/nas/results
WATCH_OUTPUT_DIR=

# Default execution mode for watch-folder jobs (pipeline or agent).
WATCH_DEFAULT_MODE=pipeline

# Default job settings for watch-folder jobs.
WATCH_DEFAULT_PROVIDER=Ollama
WATCH_DEFAULT_MODEL=qwen3-vl:8b-instruct
WATCH_DEFAULT_OCR_ENGINE=EasyOCR
WATCH_DEFAULT_SCAN_MODE=Tail Only

# File stabilization delay (seconds). Wait for size to stop changing before submit.
WATCH_STABILIZE_SECONDS=3


# Make sure we don't sit there waiting for OS socket to timeout if Ollama is unresponsive
LLM_TIMEOUT_SECONDS=60